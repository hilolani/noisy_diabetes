# -*- coding: utf-8 -*-
"""How2avoid_doubledipping_keras_regression_noisy_diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ehWaWudy4J7FFaBeIczhVf1r8UCH4HXs

# **Kerasを用いた深層ニューラルネットワークによる回帰**

Scikit Learnの**医療系toy dataset、diabetes(糖尿病)**

http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes

は**回帰問題向き**です。このコンテンツは、深層ニューラルネットワークによる、KerasRegressorを利用した解法の実例としてshibuiwilliam
により、GitHubに公開されているものを引用、修正、増補したものです。

https://qiita.com/cvusk/items/33867fbec742bda3f307

https://github.com/shibuiwilliam/keras_regression_sample/blob/master/keras_regression.ipynb

よってメインのコンテンツはモデルの設計面ではこのソースを引用しつつ、赤間がColab用に独自に書き直したものになります。なお実行には時間がかかるので、授業中は深層学習を使わず、線形重回帰分析で済ませるかもしれません。

###

## 糖尿病データ

442人の糖尿病患者それぞれについて、年齢、性別、肥満度、平均血圧、および6つの血中血清測定の10個のベースラインの独立変数を与えてあります。

目的変数としては、ベースラインから1年後の疾患進行の定量的指標を注目すべき反応として与えてあります。

なおこのデータは既に正規化されているので、Scalorを用いる必要はありません。

cf.

https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset

https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html

References:

Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least Angle Regression,” Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)

ただし、このデータセットはtoy datasetにありがちな精度の異様な高さが特徴的であり、機械学習のトレーニングにはあまり向かないと考えています。Scikit-learnを元にして、糖尿病データのノイズを含んだ私家版でこれと同型なものを作ることにしました。これはdataやtargetに正規ノイズや一様ノイズを加えたり、データ拡張(data augmentation)したりしたものです。

###

## ノイズを加えた糖尿病データ

**授業で利用するのは、sklearn.datasets.load_diabetes()のようなAPI互換の私家版 noisy toy datasetで、完全に公開されたデータとコードを用いているので、Cloud上での操作は問題はありません。またその意味でGitHubの公開リポジトリをColab上で学習者が!git cloneすることが可能となっています。**

ここでは、Scikit-learnのデータセットのように、オブジェクト指向でBunchを返すようにして、load_diabetes()のような形でデータをロードし、Pandasで利用できるような形にしました。**赤間のGitHubの公開リポジトリ**から読み込むことができます。

https://github.com/hilolani/noisy_diabetes

このリポジトリは、Colab上では、

**!pip install git+https://github.com/hilolani/noisy_diabetes.git**

によってインストールすることができます。

雑音を含む私家版データセットの作り方は以下の通りです。データに平均0,標準偏差0.1のガウシアンノイズを加え、データの精度を敢えて落としました。さらにデータ拡張(data augmentation)を想定した反復測定をシミュレートするために、ガウシアンノイズを加えてできたこのデフォルトの行データに対し、5回繰り返して一定の([-0.001, 0.001]の範囲で)一様ノイズを加えました。さらに生成した各行に対応するparticipantsとsessionsのナンバリングを追加しています。 このため、Pandasのto_csv()関数をindex=falseのオプション付きで利用し、read_csv()関数を使って読み取る際、diabetesのデータセットの既存のfeature namesに加え、dtype={'participants': int, 'sessions': int}を追加指定しています。

厳密に言うとこのノイズの加え方は正しくなく、GitHubの生データを使い、以下のように平均・標準偏差を求めて逆変換する必要があります。しかし、ここでは簡便さを重視してこの方法は取りませんでした。

df_raw = pd.read_csv("diabetes_data_raw_revised.csv", header=None)

X_raw = df_raw.iloc[:, :-1]

y_raw = df_raw.iloc[:, -1]

mean = X_raw.mean(axis=0)

std = X_raw.std(axis=0)

X_recovered = X_std * std.values + mean.values

また、目的変数も以下のようにしてノイズを加えています。追加5セッションのノイジーターゲットインデックス値'y_noisy'を作成する方法ですが、一様分布[-2, 2]の範囲に従うランダムな整数を使用して、各オリジナルのy値に追加しています。

**このGitHubリポジトリを立ち上げた目的は、誤ったデータ拡張で機械学習をモデル化すると、魔術的なブードゥー相関関係（Kriegeskorte）、ダブルディッピング、情報漏洩が起こり、結果として間違った結果が得られることを学習者に確認させるためです。ここでは、データ拡張は、同じ実験参加者に対する反復測定の結果として実現されると仮定すします。ノイズによるデータ拡張の例はnoisy_diabetes/noisyy_diabetes.pyに記述されており、実際の疑似データセットはnoisy_diabetes/noisyy_diabetes/dataに格納されています。**

**このようにして、系統的なノイズで補強された糖尿病のパブリックトイデータに、最も単純な多層パーセプトロンを適用する基本的な方法を通して、以下のことを実証しました。具体的には、特定の関心データ領域に焦点を当て、選択的にデータ拡張を適用する部分調整型ダブルディッピングは、データセット全体に一様にデータ拡張を適用する従来のダブルディッピングよりも、魔法的ブードゥー相関を成功裏にイカサマを達成する上で効果的であるということです。 **

Scikit Learn's medical toy dataset, diabetes

http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes

is suited for regression problems. This content is a quoted, modified, and expanded version of an example of a solution using KerasRegressor with a deep neural network by shibuiwilliam, published on GitHub.

https://qiita.com/cvusk/items/33867fbec742bda3f307

https://github.com/shibuiwilliam/keras_regression_sample/blob/master/keras_regression.ipynb

Therefore, the main contents are based on this source for the design of the model and rewritten by Akama for Colab.

###
Diabetes Data
For each of the 442 diabetic patients, we are given 10 baseline independent variables of age, gender, body mass index, mean blood pressure, and six blood serum measures.
The objective variable is a quantitative measure of disease progression from baseline to one year as the response of interest.
Note that this data is already normalized, so there is no need to use Scalor.

cf.
https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset
https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html

References.

Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least Angle Regression,” Annals of Statistics ( with discussion), 407-499.
(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)

However, this dataset is characterized by the oddly high accuracy typical of toy datasets, and we do not think it is suitable for machine learning training. This is a data augmentation (data augmentation) or adding normal noise or uniform noise to data or target.
###
Diabetes data with noise added

We will use a private API-compatible noisy toy dataset like sklearn.datasets.load_diabetes() in class, which uses fully public data and code, so manipulating it on the Cloud is not a problem. Also in that sense, it is possible for the learner to !git clone the public repository on GitHub on Colab.
Here, we have made it object-oriented and return Bunch, like a Scikit-learn dataset, and loaded the data in a form like load_diabetes(), which can then be used in Pandas. You can load it from the public GitHub repository, which is the following.

https://github.com/hilolani/noisy_diabetes

This repository can be installed on Colab by running,

!pip installgit+https://github.com/hilolani/noisy_diabetes.git

The following is how to create a private dataset containing noise. Gaussian noise with a mean of 0 and standard deviation of 0.1 is added to the data to reduce the accuracy of the data. To simulate repeated measurements for data augmentation, we added a constant (in the range of [-0.001, 0.001]) uniform noise to this default row data, which was created by adding Gaussian noise, for 5 iterations. We also added the numbering of participants and sessions corresponding to each generated row. For this reason, we used Pandas' to_csv() function with the index=false option, and when reading using the read_csv() function, in addition to the existing feature names in the diabetes data set, we added dtype={'participants': int, 'sessions': int} in addition to the existing feature names in the diabetes dataset.

Strictly speaking, this addition of noise is not correct and requires using the raw data from GitHub and back-transforming to obtain the mean and standard deviation as follows. However, we did not use this method here for the sake of simplicity.

df_raw = pd.read_csv(“diabetes_data_raw_revised.csv”, header=None)

X_raw = df_raw.iloc[:, :-1]

y_raw = df_raw.iloc[:, -1]

mean = X_raw.mean(axis=0)

std = X_raw.std(axis=0)

X_recovered = X_std * std.values + mean.values

The objective variable is also noisy as follows. To create the additional 5 session noisy target index value 'y_noisy', we used a random integer that follows a uniform distribution [-2, 2] range and adding it to each original y value.

The purpose of setting up this GitHub repository is to confirm for the learner that modeling machine learning with the wrong data extension can lead to magical voodoo correlation (Kriegeskorte), double dipping, and information leakage, resulting in wrong results. Here we assume that data augmentation is realized as a result of repeated measurements on the same experimental participant. An example of data augmentation by noise is described in noisy_diabetes/noisyyy_diabetes.py and the actual pseudo-dataset is stored in noisy_diabetes/noisyyy_diabetes/data.

*Through this basic method of applying the simplest multilayer perceptron to diabetes public toy data augmented with systematic noise, we have demonstrated the following. Specifically, that partial-tuned double-dipping, which focuses on specific data regions of interest and selectively applies data expansion, is more effective than traditional double-dipping, which applies data expansion uniformly to the entire data set, in successfully achieving magical voodoo correlation cheating . *

## Scikerasについて赤間よりワーニング

shibuiwilliamのコードをGoogle Colabで実行するためにScikerasを使用します。原作者のコードそのままではGoogle Colabで使えません。

from keras.wrappers.scikit_learn import KerasRegressor
が通らないので、その代りに、KerasのモデルをScikit-learnのエスティメーター（Estimator）としてラップ（包む）できるScikerasを使います。

著作権を尊重するために終わりに元のコードを掲載していますが、赤間が書き換えたものを実行します。そのままでも通る部分が多いですが、Scikerasの仕様上warningが出ます。

Scikeras は、Keras と Scikit-learn の統合を容易にするためのライブラリです。Kerasはディープラーニングのフレームワークであり、Scikit-learnは機械学習のためのツールキットですが、Scikerasはこの両者をシームレスに連携させるために使われます。Kerasで構築したディープラーニングモデルを、Scikit-learnの一貫したインターフェースで使用できるようになります。

See

**https://stackoverflow.com/questions/77104125/no-module-named-keras-wrappers**

**https://adriangb.com/scikeras/stable/**

SciKerasのドキュメントによると

「scikerasのゴールは、Keras/TensorFlowをsklearnで使えるようにすることです。これは、Scikit-Learnインターフェイスを持つKerasのラッパーを提供することで達成されます。SciKerasはkeras.wrappers.scikit_learnの後継であり、TensorFlowバージョンのラッパーよりも多くの改善を提供しています。」
"""

from google.colab import drive
drive.mount('/content/drive/')
#この後のColabの使い方についてはパワーポイント教材を参考にしてください。
from google.colab import files
#Google Driveにファイルをアップロードしたりする場合に確実なインタフェースを提供します。

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pds
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline

!pip install scikeras

!pip install japanize-matplotlib
#日本語の文字フォントを入れる

# Commented out IPython magic to ensure Python compatibility.
import japanize_matplotlib
sns.set_style('whitegrid')
japanize_matplotlib.japanize()#日本語の文字フォントが使えるようにする
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
!pip install git+https://github.com/hilolani/noisy_diabetes.git
#赤間のGitHubリポジトリからnoisyな糖尿病データをインストール
"""
上手く行かない場合は次をお試しください。
!git clone https://github.com/hilolani/noisy_diabetes.git
# %pwd
# %cd noisy_diabetes
!pip install .
"""

from scikeras.wrappers import KerasRegressor

# import libraries
import numpy as np
import pandas as pds
import tensorflow as tf
from tensorflow import keras
from keras import Sequential, layers
#from keras.wrappers.scikit_learn import KerasRegressor
#これは原作者のコードにありますが、Google Colabでは通りません。
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_diabetes

from noisy_diabetes import *

noisy_diabetes = load_noisy_diabetes()
pds.DataFrame(noisy_diabetes.data, columns=("age", "sex", "bmi", "map", "tc", "ldl", "hdl", "tch", "ltg", "glu"))

# load them to X and Y
X = np.array(noisy_diabetes.data)
Y = noisy_diabetes.target
# show their shapes
X.shape, Y.shape

Y

"""# **Scikerasを用いた深層学習モデル**

ネットワーク図を描くために

https://mennmabacon.hatenablog.com/entry/2021/11/30/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E5%9B%B3%E3%82%92%E6%8F%8F%E3%81%8F%E3%81%9F%E3%82%81%E3%81%AEPython%E3%82%B3%E3%83%BC

にある実装を利用させていただく
"""

"""
ScikerasはKerasのSequentialモデルを定義するときに、
Denseレイヤーにinput_dimを直接渡すのではなく、
Inputレイヤーを明示的に使用して入力形状を指定することを推奨しています。
"""

def reg_model2():
    # Input layerを明示的に定義
    inputs = keras.Input(shape=(10,))

    # Hidden layers
    x = keras.layers.Dense(10, activation='relu')(inputs)
    x = keras.layers.Dense(16, activation='relu')(x)

    # Output layer
    outputs = keras.layers.Dense(1)(x)

    # Modelの定義
    model2 = keras.Model(inputs=inputs, outputs=outputs)

    # コンパイル
    model2.compile(loss="mean_squared_error", optimizer="adam")
    #ここでは損失関数として、平均二乗誤差（MSE：Mean Squared Error）を使う。
    #各データに対して平均二乗誤差（MSE：Mean Squared Error）すなわち予測値と正解値の差である誤差の二乗を計算し、
    #その総和をデータ数で割った値（平均値）を出力する関数を使っている。
    return model2

# モデルを作成
model = reg_model2()

# モデルのサマリーを表示
model.summary()

from itertools import product
plt.rcParams['figure.dpi'] = 200

def make_nodes(num):
  return list(np.arange(-(num-1)/2, (num-1)/2+1,1))

def neural_network_img(*nodes_nums):
  n = len(nodes_nums)
  nodes_list = [make_nodes(num) for num in nodes_nums]
  edge_list = [list(product(nodes_list[i],nodes_list[i+1])) for i in range(n-1)]

  plt.figure()
  for i, edges in enumerate(edge_list):
    for edge in edges:
      plt.plot([i,i+1], edge, color='gray', linewidth=0.1)
  for i, nodes in enumerate(nodes_list):
    plt.scatter(np.full_like(nodes,i),nodes,c='white',edgecolors='gray')
  plt.xticks(ticks=list(range(n)),labels=['input']+[f'layer{i}' for i in range(n-2)]+['output'])
  plt.tick_params(bottom=False,
               left=False,
               right=False,
               top=False,
               labelbottom=True,
               labelleft=False,
               labelright=False,
               labeltop=False)
  ax = plt.gca()
  ax.spines['right'].set_visible(False)
  ax.spines['top'].set_visible(False)
  ax.spines['left'].set_visible(False)
  ax.spines['bottom'].set_visible(False)
  ax.set_title('Fully Coupled Neural Network')
  plt.show()

neural_network_img(10,16,1)

"""# **交差評価(Cross-validation)を利用したモデルの実行と評価**

赤間が交差評価の層化におけるデータ番号を見える化し、回帰の結果を散布図に表してそれを評価したもの。
"""

kf = KFold(n_splits=5, random_state=0, shuffle=True)
kf.get_n_splits(X, Y)
print(kf)
y_index_list = []
y_test_list = []
y_pred_list = []

X = pds.DataFrame(X)
Y = pds.DataFrame(Y)
#参加者番号を取得するためにPandasのDataFrameに変換する

for i, (train_index, test_index) in enumerate(kf.split(X, Y)):
    print(f"Fold {i}:")
    print(f"  Train: index={train_index}")
    print(f"  Test:  index={test_index}")
    X_train = X.iloc[train_index]
    X_test = X.iloc[test_index]
    y_train = Y.iloc[train_index]
    y_test = Y.iloc[test_index]
    y_train=np.reshape(y_train,(-1))
    y_test=np.reshape(y_test,(-1))
    estimator = KerasRegressor(model=reg_model2, epochs=100, batch_size=10, verbose=0)
    estimator.fit(X_train, y_train)
    y_pred = estimator.predict(X_test)
    print(f"実測値 ={y_test}")
    print(f"予測値 ={y_pred}")
    mse = mean_squared_error(y_test, y_pred)
    print("KERAS REG RMSE : %.2f" % (mse ** 0.5))
    y_index_list.append(test_index)
    y_test_list.append(y_test)
    y_pred_list.append(y_pred)
    print()

def flatten(lst):
    return np.concatenate(lst)
#ネストされたリストのネストを外す関数

index_list = flatten(y_index_list)
#print(len(index_list))
#print(index_list)
print()
y_val_list = flatten(y_test_list)
#print(len(y_val_list))
#print(y_val_list)
print()
y_pred_val_list = flatten(y_pred_list)
#print(len(y_pred_val_list))
#print(y_pred_val_list)

df = pds.DataFrame([y_val_list, y_pred_val_list]).T#Pandasのデータフレーム化
df_new = df.rename(columns={0: '実測値', 1:'予測値'})#列名の変更
sns.scatterplot(x='実測値',y='予測値',data=df_new)
plt.title('実測値と予測値の散布図')
plt.xlabel('実測値')
plt.ylabel('予測値')
z = np.polyfit(y_val_list, y_pred_val_list, 1)#線形回帰直線の計算
p = np.poly1d(z)
plt.plot(y_val_list, p(y_val_list), "r--")
plt.show()
print(f"回帰直線: y={p}")
res=df_new.corr() #PandasのDataFrameで相関係数を計算する
print(f"相関係数:{res}")

from scipy.stats import t

r = 0.376345
n = 442
#無相関検定をやってみる
t_stat = r * np.sqrt((n - 2) / (1 - r ** 2))
p_val = 2 * t.sf(np.abs(t_stat), df=n - 2)
print(p_val)
#2.549145649539668e-16なので相関はあるが、不満。

"""# **誤ったデータ拡張--二度漬けもしくは魔術的(ブードゥー)相関（その１）**

A君のやり方。データ拡張のため、反復測定のすべてのデータを実験参加者の区別なく投入したもの。トレーニングセット、テストセットの双方に同一患者からのデータが入っているのでdouble dippingになる。
"""

total_noisy_diabetes = load_total_noisy_diabetes()
df_total = pds.DataFrame(total_noisy_diabetes.data, columns=("participants", "sessions", "age", "sex", "bmi", "map", "tc", "ldl", "hdl", "tch", "ltg", "glu"))
df_total

pds.DataFrame(total_noisy_diabetes.target, columns=(["index"]))

"""# A君の方法、ここまでは良かったが。"""

df_total.loc[:, 'age':'glu']

X = np.array(df_total.loc[:, 'age':'glu'])
Y = flatten(np.array(total_noisy_diabetes.target))#good.
# show their shapes
X.shape, Y.shape

X

kf = KFold(n_splits=5, random_state=0, shuffle=True)
kf.get_n_splits(X, Y)
#print(kf)
y_index_list = []
y_test_list = []
y_pred_list = []

X = pds.DataFrame(X)
Y = pds.DataFrame(Y)
#参加者番号を取得するためにPandasのDataFrameに変換する

for i, (train_index, test_index) in enumerate(kf.split(X, Y)):
    print(f"Fold {i}:")
    X_train = X.iloc[train_index]
    X_test = X.iloc[test_index]
    y_train = Y.iloc[train_index]
    y_test = Y.iloc[test_index]
    y_train=np.reshape(y_train,(-1))
    y_test=np.reshape(y_test,(-1))
    estimator = KerasRegressor(model=reg_model2, epochs=100, batch_size=10, verbose=0)
    estimator.fit(X_train, y_train)
    y_pred = estimator.predict(X_test)
    print(f"実測値 ={y_test}")
    print(f"予測値 ={y_pred}")
    mse = mean_squared_error(y_test, y_pred)
    print("KERAS REG RMSE : %.2f" % (mse ** 0.5))
    y_index_list.append(test_index)
    y_test_list.append(y_test)
    y_pred_list.append(y_pred)
    #print()

index_list = flatten(y_index_list)
#print(len(index_list))
#print(index_list)
#print()
y_val_list = flatten(y_test_list)
#print(len(y_val_list))
#print(y_val_list)
#print()
y_pred_val_list = flatten(y_pred_list)
#print(len(y_pred_val_list))
#print(y_pred_val_list)

df = pds.DataFrame([y_val_list, y_pred_val_list]).T#Pandasのデータフレーム化
df_new = df.rename(columns={0: '実測値', 1:'予測値'})#列名の変更
sns.scatterplot(x='実測値',y='予測値',data=df_new)
plt.title('実測値と予測値の散布図')
plt.xlabel('実測値')
plt.ylabel('予測値')
z = np.polyfit(y_val_list, y_pred_val_list, 1)#線形回帰直線の計算
p = np.poly1d(z)
plt.plot(y_val_list, p(y_val_list), "r--")
plt.show()
print(f"回帰直線: y={p}")
res=df_new.corr() #PandasのDataFrameで相関係数を計算する
print(f"相関係数:{res}")

from scipy.stats import t

r = 0.422492
n = 2652
#無相関検定をやってみる
t_stat = r * np.sqrt((n - 2) / (1 - r ** 2))
p_val = 2 * t.sf(np.abs(t_stat), df=n - 2)
print(p_val)
#p値は2.624772854736399e-115

"""# 間違い！

# **誤ったデータ拡張--二度漬けもしくは魔術的(ブードゥー)相関(その２)**

度数分布表のすべてのbinsの高さを揃え、稀少でクリティカルな関心領域のみデータ拡張したもの。テストセットの分散についてはじめからわかっているinformation leakageのみならず、この関心領域では必然的に同一人物からデータがトレーニングセット、テストセットにまたがって配置されるので、double dippingになる。

B君がnoisy_diabetesの指標値の度数分布表を作ってみたら、150から先のbinの高さが足りない(データサイズが小さい)のに気になった。データ拡張の方法として、そこにこれまでの時系列データ(total_noisy_diabetes)からデータを補充してﾋﾞﾝの高さを揃えることを考えた。
"""

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
counts, bin_edges, patches = ax.hist(noisy_diabetes.target, bins=5)
bin_info = []

# 各ビンの範囲を表示
for i in range(len(counts)):
    left = bin_edges[i]
    right = bin_edges[i+1]
    height = counts[i]
    print(f"Bin {i+1}: {left:.2f} to {right:.2f}, Count = {int(height)}")
    bin_info.append([float(left), float(right), int(height)])
print(bin_info)

"""# 以下は誤ったデータ拡張法なのでPythonマニア以外は読まなくて良いです。"""

binwiseaugmentation = [[i[0], i[1], 130 - i[2]] for i in bin_info]#一番背の高いbin2だけ増やさない。
print(binwiseaugmentation)
#binwiseaugmentationはそれぞれのbinでどれだけデータ拡張するかサイズを求めるもの
increase = [i[2] for i in binwiseaugmentation]
print(increase)

#それぞれのbinに属する参加者番号
parindexpair = [[i[0], float(i[1])] for i in  [list(x) for x in zip(*[list(range(0, len(Y))),Y])]]
#print(parindexpair) #[[0, 151.0], [1, 75.0], [2, 141.0],...]
#len([i[0] for i in parindexpair if (i[1]>=25.00 and i[1]<89.20)]) #118, so good.
parbin0 = [i[0] for i in parindexpair if (i[1]>=binwiseaugmentation[0][0] and i[1]<binwiseaugmentation[0][1])]
parbin1 = [i[0] for i in parindexpair if (i[1]>=binwiseaugmentation[1][0] and i[1]<binwiseaugmentation[1][1])]
parbin2 = [i[0] for i in parindexpair if (i[1]>=binwiseaugmentation[2][0] and i[1]<binwiseaugmentation[2][1])]
parbin3 = [i[0] for i in parindexpair if (i[1]>=binwiseaugmentation[3][0] and i[1]<binwiseaugmentation[3][1])]
parbin4 = [i[0] for i in parindexpair if (i[1]>=binwiseaugmentation[4][0] and i[1]<=binwiseaugmentation[4][1])]

#total_noisy_diabetes.data（df_total）からsessionsが0の行を除き、各binの当該participantsの行を(candidatesとして)抽出、
#その中からランダムにincrease分の行を抜き出すとする。
#df_total

df_total_parbin0 = df_total.query('sessions != 0 and participants in @parbin0')
df_total_parbin1 = df_total.query('sessions != 0 and participants in @parbin1')
df_total_parbin2 = df_total.query('sessions != 0 and participants in @parbin2')
df_total_parbin3 = df_total.query('sessions != 0 and participants in @parbin3')
df_total_parbin4 = df_total.query('sessions != 0 and participants in @parbin4')
#df_total_parbin0

aug0 = random.sample(list(df_total_parbin0.index), increase[0])
#parbin1は増やさない。
aug2 = random.sample(list(df_total_parbin2.index), increase[2])
aug3 = random.sample(list(df_total_parbin3.index), increase[3])
aug4 = random.sample(list(df_total_parbin4.index), increase[4])

augtotal = [i.item() for i in np.sort(np.concatenate([aug0, aug2, aug3, aug4]))]
print(augtotal)#データ拡張の時に入れるセッションナンバー
originalintotal = list(df_total.query('sessions == 0').index)
print(originalintotal)#拡張前のデータ
selectedsessions = [i.item() for i in np.sort(np.concatenate([augtotal, originalintotal]))]
print(selectedsessions)#両者を合併

# 元のインデックスを 'selectedsessions' 列に
df_selected = df_total.loc[selectedsessions].copy()
df_selected["selectedsessions"] = df_selected.index
df_selected.reset_index(drop=True, inplace=True)

# 列の順序を変更して 'selectedsessions' を最初に
cols = ['selectedsessions'] + [col for col in df_selected.columns if col != 'selectedsessions']
df_selected = df_selected[cols]

df_selected

df_total_target = pds.DataFrame(total_noisy_diabetes.target.copy(), columns=(["index"]))
df_selected_target = df_total_target.loc[selectedsessions].copy()
#pds.DataFrame(df_selected_target, columns=(["index"]))
df_selected_target["selectedsessions"] = df_selected_target.index
df_selected_target.reset_index(drop=True, inplace=True)
cols = ['selectedsessions'] + [col for col in df_selected_target.columns if col != 'selectedsessions']
df_selected_target = df_selected_target[cols]

df_selected_target

selectedtarget = np.array(df_selected_target["index"])

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
counts, bin_edges, patches = ax.hist(selectedtarget, bins=5)
bin_info = []

# 各ビンの範囲を表示
for i in range(len(counts)):
    left = bin_edges[i]
    right = bin_edges[i+1]
    height = counts[i]
    print(f"Bin {i+1}: {left:.2f} to {right:.2f}, Count = {int(height)}")
    bin_info.append([float(left), float(right), int(height)])
print(bin_info)

"""# **以上は誤ったデータ拡張法なのでPythonマニア以外は読まなくて良かったです。（終わり）**"""

X = np.array(df_selected.loc[:, 'age':'glu'])
Y = selectedtarget
X.shape, Y.shape

kf = KFold(n_splits=5, random_state=0, shuffle=True)
kf.get_n_splits(X, Y)
#print(kf)
y_index_list = []
y_test_list = []
y_pred_list = []

X = pds.DataFrame(X)
Y = pds.DataFrame(Y)
#参加者番号を取得するためにPandasのDataFrameに変換する

for i, (train_index, test_index) in enumerate(kf.split(X, Y)):
    print(f"Fold {i}:")
    X_train = X.iloc[train_index]
    X_test = X.iloc[test_index]
    y_train = Y.iloc[train_index]
    y_test = Y.iloc[test_index]
    y_train=np.reshape(y_train,(-1))
    y_test=np.reshape(y_test,(-1))
    estimator = KerasRegressor(model=reg_model2, epochs=100, batch_size=10, verbose=0)
    estimator.fit(X_train, y_train)
    y_pred = estimator.predict(X_test)
    print(f"実測値 ={y_test}")
    print(f"予測値 ={y_pred}")
    mse = mean_squared_error(y_test, y_pred)
    print("KERAS REG RMSE : %.2f" % (mse ** 0.5))
    y_index_list.append(test_index)
    y_test_list.append(y_test)
    y_pred_list.append(y_pred)
    #print()

index_list = flatten(y_index_list)
#print(len(index_list))
#print(index_list)
#print()
y_val_list = flatten(y_test_list)
#print(len(y_val_list))
#print(y_val_list)
#print()
y_pred_val_list = flatten(y_pred_list)
#print(len(y_pred_val_list))
#print(y_pred_val_list)

df = pds.DataFrame([y_val_list, y_pred_val_list]).T#Pandasのデータフレーム化
df_new = df.rename(columns={0: '実測値', 1:'予測値'})#列名の変更
sns.scatterplot(x='実測値',y='予測値',data=df_new)
plt.title('実測値と予測値の散布図')
plt.xlabel('実測値')
plt.ylabel('予測値')
z = np.polyfit(y_val_list, y_pred_val_list, 1)#線形回帰直線の計算
p = np.poly1d(z)
plt.plot(y_val_list, p(y_val_list), "r--")
plt.show()
print(f"回帰直線: y={p}")
res=df_new.corr() #PandasのDataFrameで相関係数を計算する
print(f"相関係数:{res}")

from scipy.stats import t

r = 0.438819#これは大きな値！
n = 650
#無相関検定をやってみる
t_stat = r * np.sqrt((n - 2) / (1 - r ** 2))
p_val = 2 * t.sf(np.abs(t_stat), df=n - 2)
print(p_val)
#相関係数は5.679333879882503e-32

"""# 間違い！
しかし以下のことがわかった。

**特定の関心データ領域に焦点を当て、選択的にデータ拡張を適用する部分調整型ダブルディッピングは、データセット全体に一様にデータ補強を適用する従来のダブルディッピングよりも、魔法的ブードゥー相関というイカサマを成功裏に達成する上で効果的である**。

# **正しいデータ拡張法による回帰モデル**

これはデータサイズと分布の元々の偏りにより、さほど効果はありませんが、double dippingの無い正しい方法である。データ拡張は交差評価の各foldにおいてトレーニングセットにだけ適用されるが、テストセットは1実験参加者に対し、1データの原則が守られる。
"""

kf = KFold(n_splits=5, random_state=0, shuffle=True)
kf.get_n_splits(X, Y)
print(kf)
y_index_list = []
y_test_list = []
y_pred_list = []


X = np.array(noisy_diabetes.data)
Y = noisy_diabetes.target

X = pds.DataFrame(X)
Y = pds.DataFrame(Y)

df_total = pds.DataFrame(total_noisy_diabetes.data, columns=("participants", "sessions", "age", "sex", "bmi", "map", "tc", "ldl", "hdl", "tch", "ltg", "glu"))
X_total = np.array(df_total.loc[:, 'age':'glu'])
Y_total = flatten(np.array(total_noisy_diabetes.target))#good.

#training, testのdata splitはX, Yという実験参加者ベースで行い、
#trainingだけ、training用の実験参加者Xに対応するX_totalからの全セッションと,それらに対応するY_totalからのtarget valuesを使う。
#testは、data augmentationをしていない、X, Yからの値を使う

for i, (train_index, test_index) in enumerate(kf.split(X, Y)):#X,Yのままsplit
    print(f"Fold {i}:")
    print(f"train_participant_index: {train_index}")
    print(f"X_train_participant_index_size: {len(train_index)}")
    #X_train = X.iloc[train_index]
    #これはtraining用の実験参加者の全sessionsデータを使う
    #train_indexのparticipants numberに相当する全sessionsを抽出する。
    #train_indexと同じparticipants番号を含む行を検索した上で、participants行、sessions行をはずせばよい。
    selected_train_sessions = df_total.query('participants in @train_index')
    X_train = np.array(selected_train_sessions.loc[:, 'age':'glu'])
    print(f"X_train: {X_train}")
    print(f"X_train_size: {len(X_train)}")

    X_test = X.iloc[test_index]
    #これはtest用の実感参加者の最初のデータだけでそのまま使う

    #y_train = Y.iloc[train_index]
    #これはtraining用の実験参加者の全sessionsデータに対応するターゲット値を使う
    #selected_train_sessionsのsessionsナンバーをすべて抽出し、
    #Y_totalのうち、その位置にある要素をy_trainとして列挙
    selected_train_session_numbers = list(selected_train_sessions.index)
    y_train = [float(j) for j in [Y_total[i] for i in selected_train_session_numbers]]
    print(f"y_train: {y_train}")
    print(f"y_train_size: {len(y_train)}")

    y_test = Y.iloc[test_index]
    #これはそのまま使う


    y_train=np.reshape(y_train,(-1))
    y_test=np.reshape(y_test,(-1))
    estimator = KerasRegressor(model=reg_model2, epochs=100, batch_size=10, verbose=0)
    estimator.fit(X_train, y_train)
    y_pred = estimator.predict(X_test)
    print(f"実測値 ={y_test}")
    print(f"予測値 ={y_pred}")
    mse = mean_squared_error(y_test, y_pred)
    print("KERAS REG RMSE : %.2f" % (mse ** 0.5))
    y_index_list.append(test_index)
    y_test_list.append(y_test)
    y_pred_list.append(y_pred)
    #print()

index_list = flatten(y_index_list)
#print(len(index_list))
#print(index_list)
#print()
y_val_list = flatten(y_test_list)
#print(len(y_val_list))
#print(y_val_list)
#print()
y_pred_val_list = flatten(y_pred_list)
#print(len(y_pred_val_list))
#print(y_pred_val_list)

df = pds.DataFrame([y_val_list, y_pred_val_list]).T#Pandasのデータフレーム化
df_new = df.rename(columns={0: '実測値', 1:'予測値'})#列名の変更
sns.scatterplot(x='実測値',y='予測値',data=df_new)
plt.title('実測値と予測値の散布図')
plt.xlabel('実測値')
plt.ylabel('予測値')
z = np.polyfit(y_val_list, y_pred_val_list, 1)#線形回帰直線の計算
p = np.poly1d(z)
plt.plot(y_val_list, p(y_val_list), "r--")
plt.show()
print(f"回帰直線: y={p}")
res=df_new.corr() #PandasのDataFrameで相関係数を計算する
print(f"相関係数:{res}")

"""# **著作権を配慮したコード引用記載**

**次のセルは著作権の尊重のため、掲載するだけで実行はしません。**
"""

#このセルは原作者のコードを尊重するために掲載しますが実行はしません。

# create regression model
def reg_model():
    model = Sequential()
    model.add(Dense(10, input_dim=10, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1))

    # compile model
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model
    #ここでは損失関数として、平均二乗誤差（MSE：Mean Squared Error）とは、各データに対して平均二乗誤差（MSE：Mean Squared Error）すなわち予測値と正解値の差である誤差の二乗を計算し、その総和をデータ数で割った値（平均値）を出力する関数を使っている。

#このセルは原作者のコードを尊重するために掲載しますが実行はしません。
#通りますがwarningが出ます。
"""
/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
"""
# show the model summary
reg_model().summary()

#このセルは原作者のコードを尊重するために掲載しますが実行はしません。
#通りますが以下のwarningが出ます。
"""
/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.
  X, y = self._initialize(X, y)
/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
 """

# use data split and fit to run the model
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)
estimator = KerasRegressor(build_fn=reg_model, epochs=100, batch_size=10, verbose=0)
estimator.fit(x_train, y_train)
y_pred = estimator.predict(x_test)

# show its root mean square error
mse = mean_squared_error(y_test, y_pred)
print("KERAS REG RMSE : %.2f" % (mse ** 0.5))

#このセルは原作者のコードを尊重するために掲載しますが実行はしません。
#通りますが以下のwarningが出ます。
"""
/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.
  X, y = self._initialize(X, y)
/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
 """
# use Kfold and cross validation to run the model
seed = 7
np.random.seed(seed)
estimator = KerasRegressor(build_fn=reg_model, epochs=100, batch_size=10, verbose=0)
#授業中時間がかかるかもしれません。 KERAS REG RMSE : 55.79
#estimator = KerasRegressor(build_fn=reg_model, epochs=10, batch_size=10, verbose=0)
#時間短縮のため、epochsを10に減らすと、KERAS REG RMSE : 137.16
#kfold = KFold(n_splits=10, random_state=seed)
kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
# show its root mean square error
results = cross_val_score(estimator, X, Y, scoring='neg_mean_squared_error', cv=kfold)
mse = -results.mean()
print("KERAS REG RMSE : %.2f" % (mse ** 0.5))